{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCB_step2",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nlpbokproject/NLP_BOK_Team/blob/master/NCB_step2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86VWhqsIL2Hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import math\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
        "k =0.5\n",
        "\n",
        "input_file = pd.read_csv(\"./DATA/NBC/NBC-merged.csv\").dropna()\n",
        "input_file['date']=input_file['date']\n",
        "#input_file = pd.read_csv(\"./DATA/NBC/NBC-yeonhap_info_step1-1.csv\")\n",
        "training_set = np.array(input_file[['ngram+unigram','label']])\n",
        "#print(training_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O15kxwwCL4EY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = input_file[input_file.label=='1']\n",
        "df[['date']]\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCug6BsTL6LN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 범주에 속하는 토큰수 세기 1(예. 긍정), 0(예. 부정))\n",
        "doccnt1 = 0\n",
        "doccnt0 = 0\n",
        "\n",
        "#input_file = pd.read_csv(file,lineterminator='\\n')\n",
        "    \n",
        "# 토큰별로 문서내 빈도수 카운팅\n",
        "wordfreq = defaultdict(lambda : [0, 0])\n",
        "for doc, point in training_set:\n",
        "    words = doc.replace('@@@',',').split('.')\n",
        "    \n",
        "    for word in words :\n",
        "        \n",
        "        #my_df['words']=word\n",
        "        if point == '1' :\n",
        "            wordfreq[word][1] += 1\n",
        "            #print(word+\" 1\")\n",
        "        elif point == '-1' :\n",
        "            #print(word + \" : \"  + point)\n",
        "            wordfreq[word][0] += 1\n",
        "            #wordfreq[word][0] += 1\n",
        "        else:\n",
        "            print(\"nothing : \"+word+\" \"+point)\n",
        "            \n",
        "for key, (cnt0, cnt1) in wordfreq.items() :\n",
        "    if cnt1 > 0 :\n",
        "        doccnt1 += cnt1\n",
        "    if cnt0 > 0 :\n",
        "        doccnt0 += cnt0\n",
        "        \n",
        "        \n",
        "pd.DataFrame.from_dict(wordfreq, orient='index').to_csv('./DATA/NBC/wordfreq.csv')\n",
        "#print(wordfreq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPxoDOpBL6N8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv('./DATA/NBC/wordfreq.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-7httsQL6Uk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 범주에 속하는 토큰수 세기 1(예. 긍정), 0(예. 부정))\n",
        "doccnt1 = 0\n",
        "doccnt0 = 0\n",
        "\n",
        "#input_file = pd.read_csv(file,lineterminator='\\n')\n",
        "    \n",
        "# 토큰별로 문서내 빈도수 카운팅\n",
        "wordfreq = defaultdict(lambda : [0, 0])\n",
        "for doc, point in training_set:\n",
        "    words = doc.replace('@@@',',').split('.')\n",
        "    \n",
        "    for word in words :\n",
        "        \n",
        "        #my_df['words']=word\n",
        "        if point == '1' :\n",
        "            wordfreq[word][1] += 1\n",
        "            #print(word+\" 1\")\n",
        "        elif point == '-1' :\n",
        "            #print(word + \" : \"  + point)\n",
        "            wordfreq[word][0] += 1\n",
        "            #wordfreq[word][0] += 1\n",
        "        else:\n",
        "            print(\"nothing : \"+word+\" \"+point)\n",
        "            \n",
        "for key, (cnt0, cnt1) in wordfreq.items() :\n",
        "    if cnt1 > 0 :\n",
        "        doccnt1 += cnt1\n",
        "    if cnt0 > 0 :\n",
        "        doccnt0 += cnt0\n",
        "        \n",
        "        \n",
        "pd.DataFrame.from_dict(wordfreq, orient='index').to_csv('./DATA/NBC/wordfreq.csv')\n",
        "#print(wordfreq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvN0kMSHL6WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#190527 새로운 NBC-merged에 대해 수정\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "# training : 토큰별 조건부 확률 계산\n",
        "intensity_cutoff=1.3\n",
        "intensity_cutoff2=0.76\n",
        "eps = 1e-6\n",
        "\n",
        "\n",
        "scores = []\n",
        "wordprobs = defaultdict(lambda : [0, 0])\n",
        "for key, (cnt0, cnt1) in wordfreq.items():\n",
        "    wordprobs[key][0] = (cnt0 + k) / (doccnt0 + 2*k)\n",
        "    wordprobs[key][1] = (cnt1 + k) / (doccnt1 + 2*k)\n",
        "    \n",
        "    pos_score = wordprobs[key][1]\n",
        "    neg_score = wordprobs[key][0]\n",
        "    \n",
        "    polarity = pos_score - neg_score\n",
        "    polarity_score = pos_score/neg_score\n",
        "    \n",
        "\n",
        "    intensity = pos_score / (neg_score + eps) if polarity > 0 else neg_score / (pos_score + eps)\n",
        "    polarity_score = polarity_score if (intensity > intensity_cutoff) else 0\n",
        "    \n",
        "    label = 1 if polarity_score > 1 else '0'\n",
        "    label = -1 if polarity_score == 0 else label\n",
        "    #test =  {'key':key, 'polarity':polarity,'Polarity score': polarity_score, 'Intensity': intensity}\n",
        "    score = (key, cnt1, cnt0, wordprobs[key][1], wordprobs[key][0], polarity,polarity_score,intensity,label)\n",
        "    scores.append(score)\n",
        "    \n",
        "\n",
        "# print(wordprobs)\n",
        "\n",
        "# word,label,polarity,ratio,intensity\n",
        "\n",
        "dic = pd.DataFrame(scores, columns=['ngrams', 'counting_h', 'counting_d', 'p_h', 'p_d','polarity','Polarity score', 'Intensity','label'])\n",
        "\n",
        "#pd.merge(dic,file1,on='ngrams')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlsoC5UKMPKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 0.5\n",
        "\n",
        "intensity_cutoff=1.3\n",
        "intensity_cutoff2=0.76\n",
        "\n",
        "dic[\"w|pos\"] = (dic['counting_h']+k)/(dic['counting_h'].sum()+k*2)\n",
        "dic[\"w|neg\"] = (dic['counting_d']+k)/(dic['counting_d'].sum()+k*2)\n",
        "dic[\"polarity score\"] = dic[\"w|pos\"]/dic[\"w|neg\"]\n",
        "dic[\"intensity\"] = [x if x > 1 else 1/x for x in dic['polarity score']]\n",
        "\n",
        "#dic[\"intensity\"] = [x if (x < intensity_cutoff)&(x > intensity_cutoff2) else 0] \n",
        "\n",
        "#dic[\"polarity score\"] = dic[\"polarity score\"] if (dic[\"intensity\"] > intensity_cutoff)&(dic[\"intensity\"] < intensity_cutoff2) else 0\n",
        "dic[\"lable\"] = [-1 if x < 1 else 1 for x in dic['polarity score']] # lable가-1이면 비둘기파. 1이면 매파.\n",
        "\n",
        "\"\"\"dic[\"lable\"] = [1 if x > 1 else -1 for x in dic[\"polarity score\"]]\n",
        "dic[\"lable\"] = [0 if x == 0 else label for x in dic[\"polarity score\"]]\"\"\"\n",
        "#dic[\"lable\"] = [1 if polarity_score > 1 else -1]\n",
        "#dic[\"lable\"] = [0 if polarity_score == 0 else label]\n",
        "\n",
        "\n",
        "score = (key, cnt1, cnt0, wordprobs[key][1], wordprobs[key][0], polarity,polarity_score,intensity,label)\n",
        "scores.append(score)\n",
        "#dic = pd.DataFrame(scores, columns=['ngrams', 'counting_h', 'counting_d', 'p_h', 'p_d','polarity','Polarity score', 'Intensity','label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtHwwFtsMZHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic_final = dic[dic.intensity > 1.3][[\"ngrams\", \"counting_h\", \"counting_d\", \"w|pos\", \"w|neg\", \"polarity score\", \"intensity\", \"lable\"]]\n",
        "#dic_final.to_csv(\"./DATA/NBC/NBC_step2.csv\")\n",
        "#final\n",
        "\n",
        "dic_final.rename(['lable':'label'])\n",
        "\n",
        "dic_final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFzKVs4LMZVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}